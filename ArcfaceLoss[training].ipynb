{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Shopee-Product-Matching\n![Shopee](https://cdn.lynda.com/course/563030/563030-636270778700233910-16x9.jpg)\n","metadata":{}},{"cell_type":"markdown","source":"1. If you want to learn more about this amazing competition hosted by [Shopee](https://www.kaggle.com/c/shopee-product-matching), Please visit following [Shopee EDA Image AutoEncoder](https://www.kaggle.com/code/chiragtagadiya/shopee-basic-autoencoder).\n2. This Notebook contains EDA and Image AutoEncoder solution.","metadata":{}},{"cell_type":"code","source":"%config Completer.use_jedi = False","metadata":{"execution":{"iopub.status.busy":"2022-04-03T21:50:00.280711Z","iopub.execute_input":"2022-04-03T21:50:00.281263Z","iopub.status.idle":"2022-04-03T21:50:00.293052Z","shell.execute_reply.started":"2022-04-03T21:50:00.281226Z","shell.execute_reply":"2022-04-03T21:50:00.291745Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Import Packages","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/timmmaster')\nimport timm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-03T21:50:00.298487Z","iopub.execute_input":"2022-04-03T21:50:00.298679Z","iopub.status.idle":"2022-04-03T21:50:03.777554Z","shell.execute_reply.started":"2022-04-03T21:50:00.298656Z","shell.execute_reply":"2022-04-03T21:50:03.776831Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import math\nimport os\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport timm\nimport torch\nfrom torch import nn \nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F \nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom torch.optim import lr_scheduler\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom datetime import date\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom collections  import Counter\nimport math\nimport random","metadata":{"execution":{"iopub.status.busy":"2022-04-03T21:50:03.781045Z","iopub.execute_input":"2022-04-03T21:50:03.781252Z","iopub.status.idle":"2022-04-03T21:50:05.637990Z","shell.execute_reply.started":"2022-04-03T21:50:03.781222Z","shell.execute_reply":"2022-04-03T21:50:05.637274Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def seed_torch(seed=42):\n    random.seed(seed+1)\n#     torch.backends.cudnn.deterministic=True\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    np.random.RandomState(seed)\n    \nseed_torch()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T21:50:05.639069Z","iopub.execute_input":"2022-04-03T21:50:05.639313Z","iopub.status.idle":"2022-04-03T21:50:05.648651Z","shell.execute_reply.started":"2022-04-03T21:50:05.639281Z","shell.execute_reply":"2022-04-03T21:50:05.647693Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# class ArcFaceClassifier(nn.Module):\n#     def __init__(self, emb_size, output_classes):\n#         super().__init__()\n#         self.W = nn.Parameter(torch.Tensor(emb_size, output_classes))\n#         nn.init.kaiming_uniform_(self.W)\n#     def forward(self, x):\n#         # Step 1:\n#         x_norm = F.normalize(x)\n#         W_norm = F.normalize(self.W, dim=0)\n#         # Step 2:\n#         return x_norm @ W_norm\n    \n# def arcface_loss(cosine, targ, m=.4):\n#     # this prevents nan when a value slightly crosses 1.0 due to numerical error\n#     cosine = cosine.clip(-1+1e-7, 1-1e-7) \n#     # Step 3:\n#     arcosine = cosine.arccos()\n#     # Step 4:\n#     print(arcosine)\n#     print(\" ***\",F.one_hot(targ, num_classes = output_classes))\n#     arcosine += F.one_hot(targ, num_classes = output_classes) * m\n#     # Step 5:\n#     cosine2 = arcosine.cos()\n#     # Step 6:\n#     return F.cross_entropy(cosine2, targ)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T21:50:05.650748Z","iopub.execute_input":"2022-04-03T21:50:05.651078Z","iopub.status.idle":"2022-04-03T21:50:05.657027Z","shell.execute_reply.started":"2022-04-03T21:50:05.651042Z","shell.execute_reply":"2022-04-03T21:50:05.656278Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Configuration Options\n","metadata":{}},{"cell_type":"code","source":"TRAIN_DIR = '../input/shopee-product-matching/train_images'\nTEST_DIR = '../input/shopee-product-matching/test_images'\nTRAIN_CSV = '../input/crossvalidationfolds/folds.csv'\nMODEL_PATH = './'\n\n\nclass CFG:\n    seed = 123 \n    img_size = 512\n    classes = 11014\n    fc_dim = 512\n    epochs = 1\n    batch_size = 16\n    num_workers = 3\n    model_name = 'tf_efficientnet_b0'\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    scheduler_params = {\n        \"lr_start\": 3e-4,#1e-5\n        \"lr_max\": 1e-5 * batch_size,     # 1e-5 * 32 (if batch_size(=32) is different then)\n        \"lr_min\": 1e-6,\n        \"lr_ramp_ep\": 5,\n        \"lr_sus_ep\": 0,\n        \"lr_decay\": 0.8,\n    }\n    model_path='../input/21-mar-lr-large/2022-03-20_softmax_512x512_tf_efficientnet_b4.pt'\n    isTraining=True\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-03T21:50:05.658357Z","iopub.execute_input":"2022-04-03T21:50:05.658654Z","iopub.status.idle":"2022-04-03T21:50:05.713987Z","shell.execute_reply.started":"2022-04-03T21:50:05.658621Z","shell.execute_reply":"2022-04-03T21:50:05.713083Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Solution Approach\n\n* In this competition it is given that,if two or more images have **same label group** then they are **similar products.** \n* Basically we can use this information to transfer the business problem into **multi class classification** problem.\n* From Image EDA, I found out that we have **11014** different classes, and dataset is **not balanced dataset**\n* If you see below plot, we can clearly see that there are **hardly 1000 data points having more than 10 products per label.*\n* In this notebook I used **Weighted Sampler technique used in pytorch for handling imbalanced classification problem**\n","metadata":{}},{"cell_type":"code","source":"train_df=pd.read_csv('../input/shopee-product-matching/train.csv')\nlabelGroups = train_df.label_group.value_counts()\n# print(labelGroups)\nplt.figure(figsize=(15,5))\nplt.plot(np.arange(len(labelGroups)), labelGroups.values)\nplt.xlabel(\"Index for unique label_group_item\", size=12)\nplt.ylabel(\"Number of product data for label \", size=12)\nplt.title(\"label vs label frequency\", size=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T21:50:05.715251Z","iopub.execute_input":"2022-04-03T21:50:05.715623Z","iopub.status.idle":"2022-04-03T21:50:06.067677Z","shell.execute_reply.started":"2022-04-03T21:50:05.715586Z","shell.execute_reply":"2022-04-03T21:50:06.067012Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x360 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"# Create Custom DataSet","metadata":{}},{"cell_type":"code","source":"class ShopeeDataset(Dataset):\n    \n    def __init__(self, df,root_dir, isTraining=False, transform=None):\n        self.df = df\n        self.transform = transform\n        self.root_dir = root_dir\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):       \n        row = self.df.iloc[idx]\n        label = row.label_group\n        image_path = os.path.join(self.root_dir, row.image)\n        \n        # read image convert to RGB and apply augmentation\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            aug = self.transform(image=image)\n            image = aug['image']\n        \n        return image, torch.tensor(label).long()\n            \n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-03T21:50:06.068782Z","iopub.execute_input":"2022-04-03T21:50:06.069591Z","iopub.status.idle":"2022-04-03T21:50:06.077756Z","shell.execute_reply.started":"2022-04-03T21:50:06.069552Z","shell.execute_reply":"2022-04-03T21:50:06.076963Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Create Data Augmentation For training and validation Data","metadata":{}},{"cell_type":"code","source":"\ndef getAugmentation(IMG_SIZE, isTraining=False):\n    \n    if isTraining:\n        return albumentations.Compose([\n            albumentations.Resize(IMG_SIZE, IMG_SIZE, always_apply=True),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.Rotate(limit=120, p=0.75),\n            albumentations.RandomBrightness(limit=(0.09, 0.6), p=0.5),\n            albumentations.Normalize(\n                mean = [0.485, 0.456, 0.406],\n                std = [0.229, 0.224, 0.225]\n            ),\n            ToTensorV2(p=1.0)\n        ])\n    else:\n        return albumentations.Compose([\n            albumentations.Resize(IMG_SIZE, IMG_SIZE, always_apply=True),\n            albumentations.Normalize(\n                mean = [0.485, 0.456, 0.406],\n                std = [0.229, 0.224, 0.225]\n            ),\n            ToTensorV2(p=1.0)\n        ])","metadata":{"execution":{"iopub.status.busy":"2022-04-03T21:50:06.079299Z","iopub.execute_input":"2022-04-03T21:50:06.079553Z","iopub.status.idle":"2022-04-03T21:50:06.088583Z","shell.execute_reply.started":"2022-04-03T21:50:06.079519Z","shell.execute_reply":"2022-04-03T21:50:06.087289Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Build Model","metadata":{}},{"cell_type":"code","source":"class ArcFaceModule(nn.Module):\n    def __init__(self, in_features, out_features, scale, margin, easy_margin=False, ls_eps=0.0 ):\n        super(ArcFaceModule, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n        self.easy_margin=easy_margin\n        self.ls_eps=ls_eps\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n        \n        \n        \n    \n    def forward(self, input, label):\n        \n        # cosine = X.W = ||X|| .||W|| . cos(theta) \n        # if X and W are normalize then dot product X, W = will be cos theta\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        # phi = cos(theta + margin) = cos theta . cos(margin) -  sine theta .  sin(margin)\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n            \n        one_hot = torch.zeros(cosine.size(), device=CFG.device)\n        # one hot encoded\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        #  output = label == True ? phi : cosine  \n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        # scale the output\n        output *= self.scale\n        # return cross entropy loss on scalled output\n        return output, nn.CrossEntropyLoss()(output,label)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T21:50:06.090640Z","iopub.execute_input":"2022-04-03T21:50:06.090863Z","iopub.status.idle":"2022-04-03T21:50:06.104642Z","shell.execute_reply.started":"2022-04-03T21:50:06.090830Z","shell.execute_reply":"2022-04-03T21:50:06.103990Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"   \nclass ShopeeEncoderBackBone(nn.Module):\n    \n    def __init__(self,\n                     model_name='tf_efficientnet_b3',\n                     loss_fn='ArcFace',\n                     classes = CFG.classes,\n                     fc_dim = CFG.fc_dim,\n                     pretrained=True,\n                     use_fc=True,\n                     isTraining=True\n                ):\n        \n        \n        super(ShopeeEncoderBackBone,self).__init__()\n        \n        # create bottlenack backbone network from pretrained model \n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n        self.loss_fn =loss_fn\n        self.isTraining =isTraining\n        \n        # build top fc layers (Embedding that we are looking at testing time to represent the entire image)\n        if self.use_fc:\n            self.dropout = nn.Dropout(0.2)\n            self.fc = nn.Linear(in_features,fc_dim )\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self.init_params()\n            in_features = fc_dim\n        self.loss_fn = loss_fn\n        if self.loss_fn=='softmax':\n            self.final = nn.Linear(in_features, CFG.classes)\n        elif self.loss_fn =='ArcFace':\n            self.final = ArcFaceModule( in_features,\n                                        CFG.classes,\n                                        scale = 30,\n                                        margin = 0.5,\n                                        easy_margin = False,\n                                        ls_eps = 0.0)\n            \n    def forward(self, image, label):\n        features = self.get_features(image)\n        if self.isTraining:\n            logits = self.final(features, label)\n            return logits\n        else:\n            return features\n    \n    def init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias,0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n        \n        \n    def get_features(self,inp):\n        batch_dim = inp.shape[0]\n        inp = self.backbone(inp)\n        inp = self.pooling(inp).view(batch_dim, -1)\n        if self.use_fc and self.isTraining:\n            inp = self.dropout(inp)\n            inp = self.fc(inp)\n            inp = self.bn(inp)\n            \n        return inp\n    \n    \n# shoppe_label_classfier = ShopeeLabelGroupClassfier()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-03T21:50:06.108567Z","iopub.execute_input":"2022-04-03T21:50:06.109040Z","iopub.status.idle":"2022-04-03T21:50:06.123378Z","shell.execute_reply.started":"2022-04-03T21:50:06.109004Z","shell.execute_reply":"2022-04-03T21:50:06.122685Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Custom Learning Rate Scheduler","metadata":{}},{"cell_type":"code","source":"import torch \nfrom torch.optim.lr_scheduler import _LRScheduler\n\n\nclass ShopeeScheduler(_LRScheduler):\n    def __init__(self, optimizer, lr_start=5e-6, lr_max=1e-5,\n                 lr_min=1e-6, lr_ramp_ep=5, lr_sus_ep=0, lr_decay=0.4,\n                 last_epoch=-1):\n        self.lr_start = lr_start\n        self.lr_max = lr_max\n        self.lr_min = lr_min\n        self.lr_ramp_ep = lr_ramp_ep\n        self.lr_sus_ep = lr_sus_ep\n        self.lr_decay = lr_decay\n        super(ShopeeScheduler, self).__init__(optimizer, last_epoch)\n        \n    def get_lr(self):\n        if not self._get_lr_called_within_step:\n            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n                          \"please use `get_last_lr()`.\", UserWarning)\n        if self.last_epoch == 0:\n            self.last_epoch += 1\n            return [self.lr_start for _ in self.optimizer.param_groups]\n        lr = self._compute_lr_from_epoch()\n        self.last_epoch += 1\n        return [lr for _ in self.optimizer.param_groups]\n    \n    def _get_closed_form_lr(self):\n        return self.base_lrs\n    \n    def _compute_lr_from_epoch(self):\n        if self.last_epoch < self.lr_ramp_ep:\n            lr = ((self.lr_max - self.lr_start) / \n                  self.lr_ramp_ep * self.last_epoch + \n                  self.lr_start)\n        elif self.last_epoch < self.lr_ramp_ep + self.lr_sus_ep:\n            lr = self.lr_max\n        else:\n            lr = ((self.lr_max - self.lr_min) * self.lr_decay**\n                  (self.last_epoch - self.lr_ramp_ep - self.lr_sus_ep) + \n                  self.lr_min)\n        return lr","metadata":{"execution":{"iopub.status.busy":"2022-04-03T21:50:06.125574Z","iopub.execute_input":"2022-04-03T21:50:06.126292Z","iopub.status.idle":"2022-04-03T21:50:06.137434Z","shell.execute_reply.started":"2022-04-03T21:50:06.126257Z","shell.execute_reply":"2022-04-03T21:50:06.136813Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Training  Single Epoch\n","metadata":{}},{"cell_type":"code","source":"def training_one_epoch(epoch_num,model, dataloader,optimizer, scheduler, device, loss_criteria):\n    avgloss = 0.0\n    # put model in traning model\n    model.train()\n    tq = tqdm(enumerate(dataloader), total=len(dataloader))\n    y_true=[]\n    y_pred=[]\n    for idx, data in tq:\n        batch_size = data[0].shape[0]\n        images = data[0]\n        targets = data[1]\n        # zero out gradient\n        optimizer.zero_grad()\n        # put input and target to device\n        images = images.to(device)\n        targets = targets.to(device)\n        # pass input to the model\n        output,loss = model(images,targets)\n        # backpropogation \n        loss.backward()\n        # update learning rate step\n        optimizer.step() \n        predicted_label=torch.argmax(output,1)\n        y_true.extend(targets.detach().cpu().numpy())\n        y_pred.extend(predicted_label.detach().cpu().numpy())\n        # avg loss\n        avgloss += loss.item() \n\n        tq.set_postfix({'loss' : '%.6f' %float(avgloss/(idx+1)), 'LR' : optimizer.param_groups[0]['lr']})\n        \n    # lr scheduler step after each epoch\n    scheduler.step()\n    f1_score_metric = f1_score(y_true, y_pred, average='micro')\n    tq.set_postfix({'Training f1 score' : '%.6f' %float(f1_score_metric)})\n    return avgloss / len(dataloader),f1_score_metric\n    \n    \n    \n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-03T21:50:06.138832Z","iopub.execute_input":"2022-04-03T21:50:06.139262Z","iopub.status.idle":"2022-04-03T21:50:06.150175Z","shell.execute_reply.started":"2022-04-03T21:50:06.139228Z","shell.execute_reply":"2022-04-03T21:50:06.149472Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Validating Single Epoch","metadata":{}},{"cell_type":"code","source":"\n\ndef validation_one_epoch(model, dataloader, epoch, device, loss_criteria):\n    avgloss = 0.0\n    # put model in traning model\n    model.eval()\n    tq = tqdm(enumerate(dataloader), desc = \"Training Epoch { }\" + str(epoch+1))\n    \n    y_true=[]\n    y_pred=[]\n    with torch.no_grad():\n        for idx, data in tq:\n            batch_size = data[0].shape[0]\n            images = data[0]\n            targets = data[1]\n\n            images = images.to(device)\n            targets = targets.to(device)\n            output,loss = model(images,targets)\n            predicted_label=torch.argmax(output,1)\n            y_true.extend(targets.detach().cpu().numpy())\n            y_pred.extend(predicted_label.detach().cpu().numpy())\n\n            avgloss += loss.item() \n\n            tq.set_postfix({'validation loss' : '%.6f' %float(avgloss/(idx+1))})\n    f1_score_metric = f1_score(y_true, y_pred, average='micro')\n    tq.set_postfix({'validation f1 score' : '%.6f' %float(f1_score_metric)})\n    return avgloss / len(dataloader),f1_score_metric\n        \n","metadata":{"execution":{"iopub.status.busy":"2022-04-03T21:50:06.151540Z","iopub.execute_input":"2022-04-03T21:50:06.152043Z","iopub.status.idle":"2022-04-03T21:50:06.162252Z","shell.execute_reply.started":"2022-04-03T21:50:06.152008Z","shell.execute_reply":"2022-04-03T21:50:06.161555Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Helper Function for Handling class imbalanced data","metadata":{}},{"cell_type":"code","source":"import numpy as np \ndef get_class_weights(data):\n    # Word dictionary keys will be label and value will be frequency of label in dataset\n    weight_dict=Counter(data['label_group'])\n    # for each data point get label count data\n    class_sample_count= np.array([weight_dict[row[4]] for row in data.values])\n    # each data point weight will be inverse of frequency\n    weight = 1. / class_sample_count\n    weight=torch.from_numpy(weight)\n    return weight","metadata":{"execution":{"iopub.status.busy":"2022-04-03T21:50:06.163460Z","iopub.execute_input":"2022-04-03T21:50:06.164117Z","iopub.status.idle":"2022-04-03T21:50:06.174266Z","shell.execute_reply.started":"2022-04-03T21:50:06.164081Z","shell.execute_reply":"2022-04-03T21:50:06.173504Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"class ArcFace(nn.Module):\n    r\"\"\"Implement of ArcFace (https://arxiv.org/pdf/1801.07698v1.pdf):\n        Args:\n            in_features: size of each input sample\n            out_features: size of each output sample\n            device_id: the ID of GPU where the model will be trained by model parallel. \n                       if device_id=None, it will be trained on CPU without model parallel.\n            s: norm of input feature\n            m: margin\n            cos(theta+m)\n        \"\"\"\n    def __init__(self, in_features, out_features, s = 64.0, m = 0.50, easy_margin = False):\n        super(ArcFace, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n\n        self.s = s\n        self.m = m\n        \n        self.kernel = Parameter(torch.FloatTensor(in_features, out_features))\n        #nn.init.xavier_uniform_(self.kernel)\n        nn.init.normal_(self.kernel, std=0.01)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, embbedings, label):\n        embbedings = l2_norm(embbedings, axis = 1)\n        kernel_norm = l2_norm(self.kernel, axis = 0)\n        cos_theta = torch.mm(embbedings, kernel_norm)\n        cos_theta = cos_theta.clamp(-1, 1)  # for numerical stability\n        with torch.no_grad():\n            origin_cos = cos_theta.clone()\n        target_logit = cos_theta[torch.arange(0, embbedings.size(0)), label].view(-1, 1)\n\n        sin_theta = torch.sqrt(1.0 - torch.pow(target_logit, 2))\n        cos_theta_m = target_logit * self.cos_m - sin_theta * self.sin_m #cos(target+margin)\n        if self.easy_margin:\n            final_target_logit = torch.where(target_logit > 0, cos_theta_m, target_loit)\n        else:\n            final_target_logit = torch.where(target_logit > self.th, cos_theta_m, target_logit - self.mm)\n\n        cos_theta.scatter_(1, label.view(-1, 1).long(), final_target_logit)\n        output = cos_theta * self.s\n        return output, origin_cos * self.s\n","metadata":{"execution":{"iopub.status.busy":"2022-04-03T21:50:06.176867Z","iopub.execute_input":"2022-04-03T21:50:06.177051Z","iopub.status.idle":"2022-04-03T21:50:06.190637Z","shell.execute_reply.started":"2022-04-03T21:50:06.177024Z","shell.execute_reply":"2022-04-03T21:50:06.189941Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# TESTING","metadata":{}},{"cell_type":"code","source":"CFG.epochs =25","metadata":{"execution":{"iopub.status.busy":"2022-04-03T21:50:06.191948Z","iopub.execute_input":"2022-04-03T21:50:06.192366Z","iopub.status.idle":"2022-04-03T21:50:06.200325Z","shell.execute_reply.started":"2022-04-03T21:50:06.192332Z","shell.execute_reply":"2022-04-03T21:50:06.199373Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def run_training(model=None, history=None):\n    data = pd.read_csv('../input/cvfolds/folds.csv')\n    # label encoding\n    labelencoder= LabelEncoder()\n    data['label_group_original']=data['label_group']\n    data['label_group'] = labelencoder.fit_transform(data['label_group'])\n    \n    \n    \n    # create training_data and validation data initially not using k fold\n    train_data = data[data['fold']!=0]\n    validation_data = data[data['fold']==0]\n    #     train_data = data\n    # training augmentation\n    train_aug = getAugmentation(CFG.img_size,isTraining=True )\n    validation_aug = getAugmentation(CFG.img_size, isTraining=False)\n    # create custom train and validation dataset\n    \n    trainset = ShopeeDataset(train_data, TRAIN_DIR, isTraining=True, transform = train_aug)\n    validset = ShopeeDataset(validation_data, TRAIN_DIR, isTraining=False, transform = validation_aug)\n    #print(len(data))\n    #print(len(trainset))\n    # create data sampler\n                  \n    # get weights for  classes\n    samples_weight=get_class_weights(train_data)\n    \n    sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight, num_samples=len(samples_weight))   \n    \n    \n    # create custom training and validation data loader num_workers=CFG.num_workers,\n    train_dataloader = DataLoader(trainset, batch_size=CFG.batch_size,\n                          drop_last=True,pin_memory=True,sampler=sampler)\n    \n    validation_dataloader = DataLoader(validset, batch_size=CFG.batch_size,\n                        drop_last=True,pin_memory=True)\n    \n    \n    # define loss function\n    loss_criteria = nn.CrossEntropyLoss()\n    loss_criteria.to(CFG.device)\n    # define model\n    \n    if not model:\n        model = ShopeeEncoderBackBone()\n        model.to(CFG.device)\n    \n    # define optimzer\n    optimizer = torch.optim.Adam(model.parameters(),lr= CFG.scheduler_params['lr_start'])\n    \n    # learning rate scheudler\n    #scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=7, T_mult=1, eta_min=1e-6, last_epoch=-1)\n    scheduler = ShopeeScheduler(optimizer, **CFG.scheduler_params)\n    if not history:\n        history = {'train_loss':[],'validation_loss':[],'train_f1_score':[],'scheduler':[], 'valid_f1_score':[]}\n    for epoch in range(CFG.epochs):\n        \n        # get current epoch training loss\n        avg_train_loss, avg_f1_score = training_one_epoch(epoch_num = epoch,\n                                           model = model,\n                                           dataloader = train_dataloader,\n                                           optimizer = optimizer,\n                                           scheduler = scheduler,\n                                           device = CFG.device, \n                                           loss_criteria = loss_criteria)\n        print(\"Epoch : {} avg f1 {}\".format(epoch+1,avg_f1_score))\n        # get current epoch validation loss\n        avg_validation_loss, avg_valid_f1_score  = validation_one_epoch(model = model,\n                                          dataloader = validation_dataloader,\n                                          epoch = epoch,\n                                          device = CFG.device,\n                                          loss_criteria = loss_criteria)\n        \n        print(\"Epoch : {} avg Validation f1 {}\".format(epoch+1,avg_f1_score))\n        \n        history['train_loss'].append(avg_train_loss)\n        history['validation_loss'].append(avg_validation_loss)\n        history['train_f1_score'].append(avg_f1_score) \n        history['valid_f1_score'].append(avg_valid_f1_score)\n        history['scheduler'].append(scheduler.state_dict())\n        # save model\n        torch.save(model.state_dict(), MODEL_PATH +\"Train_F1_score_\"+str(avg_f1_score)+\"valid_f1_score\"+str(avg_valid_f1_score)+\"_\"+\"Epoch_\"+str(epoch)+\"_lr_start_\"+str(CFG.scheduler_params['lr_start'])+\"_lr_max_\"+str(CFG.scheduler_params['lr_max'])+'_softmax_512x512_{}.pt'.format(CFG.model_name))\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n            'scheduler': scheduler.state_dict()\n            },\n         MODEL_PATH +\"F1_score_\"+str(avg_f1_score)+\"valid_f1_score\"+str(avg_valid_f1_score)+\"_\"+\"Epoch_\"+str(epoch)+\"_lr_start_\"+str(CFG.scheduler_params['lr_start'])+\"_lr_max_\"+str(CFG.scheduler_params['lr_max'])+'_softmax_512x512_{}.pt'.format(CFG.model_name)\n        )\n    \n        \n        \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:19:26.881418Z","iopub.execute_input":"2022-04-03T22:19:26.881887Z","iopub.status.idle":"2022-04-03T22:19:26.903999Z","shell.execute_reply.started":"2022-04-03T22:19:26.881850Z","shell.execute_reply":"2022-04-03T22:19:26.902829Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"history={'train_loss':[],'validation_loss':[],'train_f1_score':[], 'scheduler':[], 'valid_f1_score':[]}\n# retraining with saved model so model and history will not be None\n# model = ShopeeEncoderBackBone()\n# model.to(CFG.device)\n# model.load_state_dict(torch.load('../input/arc-face-trained-b3-059/F1_score_0.5976635514018691_Epoch_21_lr_start_0.0003_lr_max_0.00016_softmax_512x512_tf_efficientnet_b0.pt'))\nCFG.isTraining=True\nCFG.epochs=1\nif CFG.isTraining:\n    model, history = run_training(model=None, history=None)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:19:32.199451Z","iopub.execute_input":"2022-04-03T22:19:32.200175Z","iopub.status.idle":"2022-04-03T22:48:28.629182Z","shell.execute_reply.started":"2022-04-03T22:19:32.200125Z","shell.execute_reply":"2022-04-03T22:48:28.628449Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:1802: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n  FutureWarning,\n100%|| 1712/1712 [25:16<00:00,  1.13it/s, loss=21.505441, LR=0.0003]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 1 avg f1 0.00018253504672897196\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch { }1: 428it [03:38,  1.96it/s, validation loss=19.710756]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 1 avg Validation f1 0.00018253504672897196\n","output_type":"stream"}]},{"cell_type":"code","source":"history ","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:17:15.116320Z","iopub.status.idle":"2022-04-03T22:17:15.116907Z","shell.execute_reply.started":"2022-04-03T22:17:15.116641Z","shell.execute_reply":"2022-04-03T22:17:15.116668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# daddy cool DADDY COOL23 123\n# 456789101121 Daddy Cool123456789","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:17:15.118123Z","iopub.status.idle":"2022-04-03T22:17:15.118828Z","shell.execute_reply.started":"2022-04-03T22:17:15.118560Z","shell.execute_reply":"2022-04-03T22:17:15.118590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# import torch\n# torch.cuda.empty_cache()\n# import gc\n\n\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:17:15.119983Z","iopub.status.idle":"2022-04-03T22:17:15.120528Z","shell.execute_reply.started":"2022-04-03T22:17:15.120295Z","shell.execute_reply":"2022-04-03T22:17:15.120322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot Training and Validation Loss and Accuracy","metadata":{}},{"cell_type":"code","source":"# Daddy cool  ","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:17:15.121577Z","iopub.status.idle":"2022-04-03T22:17:15.122130Z","shell.execute_reply.started":"2022-04-03T22:17:15.121896Z","shell.execute_reply":"2022-04-03T22:17:15.121922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.isTraining:\n    epoch_lst = [ i+1 for i in range(15)]\n    plt.plot(epoch_lst,history['train_loss'])\n\n    plt.xlabel(\"Epoch number\")\n    plt.ylabel('Training Loss')\n    plt.title('Training Loss ArcFace Loss Function')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:17:15.123195Z","iopub.status.idle":"2022-04-03T22:17:15.123730Z","shell.execute_reply.started":"2022-04-03T22:17:15.123506Z","shell.execute_reply":"2022-04-03T22:17:15.123531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.isTraining:\n    plt.plot(epoch_lst,history['train_f1_score'])\n    plt.xlabel(\"Epoch number\")\n    plt.ylabel('train_f1_score ')\n    plt.title('Train_f1_score Loss ArcFace Loss Function')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:17:15.124764Z","iopub.status.idle":"2022-04-03T22:17:15.125311Z","shell.execute_reply.started":"2022-04-03T22:17:15.125078Z","shell.execute_reply":"2022-04-03T22:17:15.125103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction ","metadata":{}},{"cell_type":"code","source":"def prediction(model):\n    data = pd.read_csv('../input/cvfolds/folds.csv')\n\n    # label encoding\n    labelencoder= LabelEncoder()\n    data['label_group'] = labelencoder.fit_transform(data['label_group'])\n    # Prepare Validation data\n    validation_data = data[data['fold']==0]\n    validation_aug = getAugmentation(CFG.img_size,isTraining=False)\n    validset = ShopeeDataset(validation_data, TRAIN_DIR, isTraining=False, transform = validation_aug)\n    test_data_loader = torch.utils.data.DataLoader(validset,batch_size=CFG.batch_size)\n    \n    # put model in evalution mode\n    \n    model.eval()\n    \n    tq = tqdm(enumerate(test_data_loader))\n    y_true=[]\n    y_pred=[]\n    with torch.no_grad():\n        for idx, data in tq:\n            images = data[0]\n            targets = data[1]\n            \n            images = images.to(CFG.device)\n            targets = targets.to(CFG.device)\n            y_true.extend(targets.detach().cpu().numpy())\n            output,loss = model(images,targets)\n            outputs=torch.argmax(output,1)\n            y_pred.extend(outputs.detach().cpu().numpy())\n        \n    f1_score_metric = f1_score(y_true, y_pred, average='micro')\n    return f1_score_metric\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:17:15.126373Z","iopub.status.idle":"2022-04-03T22:17:15.126909Z","shell.execute_reply.started":"2022-04-03T22:17:15.126671Z","shell.execute_reply":"2022-04-03T22:17:15.126697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1 = prediction(model)\nprint(f1)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:17:15.127974Z","iopub.status.idle":"2022-04-03T22:17:15.128519Z","shell.execute_reply.started":"2022-04-03T22:17:15.128295Z","shell.execute_reply":"2022-04-03T22:17:15.128319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if not CFG.isTraining:\nmodel = ShopeeEncoderBackBone().to(CFG.device)\nmodel.load_state_dict(torch.load('../input/arc-face-trained-b3-059/F1_score_0.5976635514018691_Epoch_21_lr_start_0.0003_lr_max_0.00016_softmax_512x512_tf_efficientnet_b0.pt'))\n#     f1=prediction(model)\n#     print(\"F1 score {}\".format(f1))","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:17:15.129569Z","iopub.status.idle":"2022-04-03T22:17:15.130112Z","shell.execute_reply.started":"2022-04-03T22:17:15.129884Z","shell.execute_reply":"2022-04-03T22:17:15.129910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = torch.load('.../input/arc-face-trained-b3-059/F1_score_0.5976635514018691_Epoch_21_lr_start_0.0003_lr_max_0.00016_softmax_512x512_tf_efficientnet_b0.pt')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:17:15.131141Z","iopub.status.idle":"2022-04-03T22:17:15.131674Z","shell.execute_reply.started":"2022-04-03T22:17:15.131448Z","shell.execute_reply":"2022-04-03T22:17:15.131473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(model1['model_state_dict'])","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:17:15.132727Z","iopub.status.idle":"2022-04-03T22:17:15.133295Z","shell.execute_reply.started":"2022-04-03T22:17:15.133056Z","shell.execute_reply":"2022-04-03T22:17:15.133081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.keys()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:17:15.134327Z","iopub.status.idle":"2022-04-03T22:17:15.134859Z","shell.execute_reply.started":"2022-04-03T22:17:15.134622Z","shell.execute_reply":"2022-04-03T22:17:15.134647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1['epoch']","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:17:15.135901Z","iopub.status.idle":"2022-04-03T22:17:15.136434Z","shell.execute_reply.started":"2022-04-03T22:17:15.136202Z","shell.execute_reply":"2022-04-03T22:17:15.136228Z"},"trusted":true},"execution_count":null,"outputs":[]}]}