{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append('../input/timmmaster')\nimport timm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-16T20:36:05.468946Z","iopub.execute_input":"2022-04-16T20:36:05.469289Z","iopub.status.idle":"2022-04-16T20:36:08.473505Z","shell.execute_reply.started":"2022-04-16T20:36:05.469198Z","shell.execute_reply":"2022-04-16T20:36:08.472636Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Preliminaries\nfrom tqdm import tqdm\nimport math\nimport random\nimport os\nimport pandas as pd\nimport numpy as np\n\n# Visuals and CV2\nimport cv2\n\n# albumentations for augs\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n#torch\nimport torch\nimport timm\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\n\n\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:36:08.476442Z","iopub.execute_input":"2022-04-16T20:36:08.477732Z","iopub.status.idle":"2022-04-16T20:36:10.682030Z","shell.execute_reply.started":"2022-04-16T20:36:08.477685Z","shell.execute_reply":"2022-04-16T20:36:10.681115Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"    If you are want to access the dataset please use this kaggle link\n[Kaggle Link](https://www.kaggle.com/chiragtagadiya/shopee-arcface-image-embedding-results/edit)","metadata":{}},{"cell_type":"code","source":"# configuration class\n\nclass CFG:\n    loss_module='ArcFace'\n    TRAIN_DIR='../input/shopee-product-matching/train_images'\n    TEST_DIR='../input/shopee-product-matching/test_images'\n    seed = 123 \n    img_size = 512\n    classes = 11014\n    fc_dim = 512\n    epochs = 25\n    batch_size = 12\n    num_workers = 3\n    model_name = 'tf_efficientnet_b3'\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model_path_arcface='../input/pretrained-b3/Train_F1_score_0.9061769859813084valid_f1_score0.4245035046728972_Epoch_0_lr_start_2.23e-05_lr_max_0.00016_softmax_512x512_tf_efficientnet_b0.pt'\n    model_path_softmax = '../input/label-classfier-model/2022-04-15_softmax_512x512_tf_efficientnet_b4.pt'\n    # # check true when we want to train the model\n    isTraining=False","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:36:10.683821Z","iopub.execute_input":"2022-04-16T20:36:10.684321Z","iopub.status.idle":"2022-04-16T20:36:10.694887Z","shell.execute_reply.started":"2022-04-16T20:36:10.684275Z","shell.execute_reply":"2022-04-16T20:36:10.693651Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Loading Data","metadata":{}},{"cell_type":"code","source":"def read_dataset():\n\n    # if not in testing phase read train dataset else test dataset\n    df = pd.read_csv('../input/shopee-product-matching/train.csv')\n    # we have information that label_group is same for similar kind of product\n    # let's use this to get F1 score for our final model\n    tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n    df['matches'] = df['label_group'].map(tmp)\n    df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n    # get cuda frame for faster GPU computation\n    df_cu = cudf.DataFrame(df)\n    \n        \n    return df, df_cu","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:36:10.699616Z","iopub.execute_input":"2022-04-16T20:36:10.700842Z","iopub.status.idle":"2022-04-16T20:36:10.714489Z","shell.execute_reply.started":"2022-04-16T20:36:10.700778Z","shell.execute_reply":"2022-04-16T20:36:10.713038Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Create Dataset\n","metadata":{}},{"cell_type":"code","source":"class ShopeeQueryDataset(Dataset):\n    \n    def __init__(self, imagePath, transform=None):\n        self.imagePath = imagePath\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.imagePath)\n    \n    def __getitem__(self, idx):\n \n        row = self.imagePath[idx]\n        # read image convert to RGB and apply augmentation\n        image = cv2.imread(row)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        # apply transformation\n        if self.transform:\n            aug = self.transform(image=image)\n            image = aug['image']\n        \n        return image, torch.tensor(1).long()\n            ","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:36:10.718827Z","iopub.execute_input":"2022-04-16T20:36:10.720271Z","iopub.status.idle":"2022-04-16T20:36:10.733298Z","shell.execute_reply.started":"2022-04-16T20:36:10.720199Z","shell.execute_reply":"2022-04-16T20:36:10.732323Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_test_transforms():\n\n    return albumentations.Compose(\n        [\n            albumentations.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n            albumentations.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:36:10.735321Z","iopub.execute_input":"2022-04-16T20:36:10.735895Z","iopub.status.idle":"2022-04-16T20:36:10.744697Z","shell.execute_reply.started":"2022-04-16T20:36:10.735848Z","shell.execute_reply":"2022-04-16T20:36:10.743600Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"## Model 1 : Product Classfier Softmax Loss","metadata":{}},{"cell_type":"code","source":"class ShopeeLabelGroupClassfier(nn.Module):\n    \n    def __init__(self,\n                     model_name='tf_efficientnet_b0',\n                     loss_fn='softmax',\n                     classes = CFG.classes,\n                     fc_dim = CFG.fc_dim,\n                     pretrained=False,\n                     use_fc=True,\n                     isTraining=False\n                ):\n        \n        \n        super(ShopeeLabelGroupClassfier,self).__init__()\n        \n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n        self.loss_fn =loss_fn\n        self.isTraining = isTraining\n        \n        if self.use_fc:\n            self.dropout = nn.Dropout(0.2)\n            self.fc = nn.Linear(in_features,fc_dim )\n            self.bn = nn.BatchNorm1d(fc_dim)\n            in_features = fc_dim\n        self.loss_fn = loss_fn\n        \n        if self.loss_fn=='softmax':\n            self.final = nn.Linear(in_features, CFG.classes)\n    \n    def forward(self, image, label):\n        features = self.get_features(image)\n        if self.loss_fn=='softmax' and CFG.isTraining:\n            logits = self.final(features)\n            return logits\n        else:\n            return features\n    \n    def get_features(self,inp):\n        batch_dim = inp.shape[0]\n        inp = self.backbone(inp)\n        inp = self.pooling(inp).view(batch_dim, -1)\n        if self.use_fc and self.isTraining:\n            inp = self.dropout(inp)\n            inp = self.fc(inp)\n            inp = self.bn(inp)\n        return inp\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:36:10.746324Z","iopub.execute_input":"2022-04-16T20:36:10.746984Z","iopub.status.idle":"2022-04-16T20:36:10.768922Z","shell.execute_reply.started":"2022-04-16T20:36:10.746907Z","shell.execute_reply":"2022-04-16T20:36:10.768015Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Model 2: Product Classfier ArcFace Loss ","metadata":{}},{"cell_type":"code","source":"class ArcFaceModule(nn.Module):\n    def __init__(self, in_features, out_features, scale, margin, easy_margin=False, ls_eps=0.0 ):\n        super(ArcFaceModule, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n        self.easy_margin=easy_margin\n        self.ls_eps=ls_eps\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n        \n        \n        \n    \n    def forward(self, input, label):\n        \n        # cosine = X.W = ||X|| .||W|| . cos(theta) \n        # if X and W are normalize then dot product X, W = will be cos theta\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        # phi = cos(theta + margin) = cos theta . cos(margin) -  sine theta .  sin(margin)\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n            \n        one_hot = torch.zeros(cosine.size(), device=CFG.device)\n        # one hot encoded\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n        #  output = label == True ? phi : cosine  \n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        # scale the output\n        output *= self.scale\n        # return cross entropy loss on scalled output\n        return output, nn.CrossEntropyLoss()(output,label)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:36:10.771896Z","iopub.execute_input":"2022-04-16T20:36:10.772329Z","iopub.status.idle":"2022-04-16T20:36:10.792072Z","shell.execute_reply.started":"2022-04-16T20:36:10.772292Z","shell.execute_reply":"2022-04-16T20:36:10.791120Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"   \nclass ShopeeEncoderBackBone(nn.Module):\n    \n    def __init__(self,\n                     model_name='tf_efficientnet_b3',\n                     loss_fn='ArcFace',\n                     classes = CFG.classes,\n                     fc_dim = CFG.fc_dim,\n                     pretrained=False,\n                     use_fc=True,\n                     isTraining=False\n                ):\n        \n        \n        super(ShopeeEncoderBackBone,self).__init__()\n        \n        # create bottlenack backbone network from pretrained model \n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n        self.loss_fn =loss_fn\n        self.isTraining =isTraining\n        \n        # build top fc layers (Embedding that we are looking at testing time to represent the entire image)\n        # this will work as regularizer\n        if self.use_fc:\n            self.dropout = nn.Dropout(0.2)\n            self.fc = nn.Linear(in_features,fc_dim )\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self.init_params()\n            in_features = fc_dim\n        self.loss_fn = loss_fn\n        if self.loss_fn=='softmax':\n            self.final = nn.Linear(in_features, CFG.classes)\n        elif self.loss_fn =='ArcFace':\n            self.final = ArcFaceModule( in_features,\n                                        CFG.classes,\n                                        scale = 30,\n                                        margin = 0.5,\n                                        easy_margin = False,\n                                        ls_eps = 0.0)\n            \n    def forward(self, image, label):\n        features = self.get_features(image)\n        if self.isTraining:\n            logits = self.final(features, label)\n            return logits\n        else:\n            return features\n    \n    def init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias,0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n        \n        \n    def get_features(self,inp):\n        batch_dim = inp.shape[0]\n        inp = self.backbone(inp)\n        inp = self.pooling(inp).view(batch_dim, -1)\n        if self.use_fc and self.isTraining:\n            inp = self.dropout(inp)\n            inp = self.fc(inp)\n            inp = self.bn(inp)\n            \n        return inp\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:36:10.793753Z","iopub.execute_input":"2022-04-16T20:36:10.794170Z","iopub.status.idle":"2022-04-16T20:36:10.811451Z","shell.execute_reply.started":"2022-04-16T20:36:10.794133Z","shell.execute_reply":"2022-04-16T20:36:10.810729Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Load trained model\n\ndef getPretrainedModel(loss_module='ArcFace', model_path=CFG.model_path_arcface, device=CFG.device) :\n    \n    if loss_module== 'ArcFace':\n        # load arcface loss classfier\n        model = ShopeeEncoderBackBone()\n        model.load_state_dict(torch.load(CFG.model_path_arcface, map_location=CFG.device))\n        model = model.to(CFG.device)\n        return model\n    else:\n        #load softmax classfier\n        model = ShopeeLabelGroupClassfier()\n        model.load_state_dict(torch.load(CFG.model_path_softmax, map_location=CFG.device))\n        model = model.to(CFG.device)\n        return model","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:36:10.815665Z","iopub.execute_input":"2022-04-16T20:36:10.815979Z","iopub.status.idle":"2022-04-16T20:36:10.823204Z","shell.execute_reply.started":"2022-04-16T20:36:10.815949Z","shell.execute_reply":"2022-04-16T20:36:10.822297Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Generate Embeddings\n","metadata":{}},{"cell_type":"code","source":"def get_images_path(df, root_dir):\n    imagepaths = [ root_dir + \"/\"+image for image in df['image'].tolist()]\n    return imagepaths\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:36:10.825205Z","iopub.execute_input":"2022-04-16T20:36:10.825499Z","iopub.status.idle":"2022-04-16T20:36:10.831720Z","shell.execute_reply.started":"2022-04-16T20:36:10.825463Z","shell.execute_reply":"2022-04-16T20:36:10.830795Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def getEmbeddings(queryImagesPath, model, transform=None):\n    # create dataset from image paths\n    query_dataset = ShopeeQueryDataset(queryImagesPath,  transform = transform)\n    \n    # create dataloader\n    query_dataloader = torch.utils.data.DataLoader(\n                                                query_dataset,\n        batch_size=16\n    )\n    \n    \n    # put model in evaluation mode\n    model.eval()\n    embeddings = []\n    with torch.no_grad():\n         \n        for idx, datax  in tqdm(enumerate(query_dataloader)):\n            image, label = datax\n            image = image.to(CFG.device)\n            label = label.to(CFG.device)\n            # forward pass to get features\n            features = model(image, label)\n            image_embeddings = features.detach().cpu().numpy()\n            embeddings.append(image_embeddings)\n            \n            \n    image_embeddings = np.concatenate(embeddings)\n            \n    return image_embeddings\n    \n    \n        ","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:36:10.833594Z","iopub.execute_input":"2022-04-16T20:36:10.834040Z","iopub.status.idle":"2022-04-16T20:36:10.842782Z","shell.execute_reply.started":"2022-04-16T20:36:10.834002Z","shell.execute_reply":"2022-04-16T20:36:10.841542Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# def get_neighbors(df, embeddings, KNN = 50, isImage=False):\n    \n\n#     # cuml neighbors\n#     model = NearestNeighbors(n_neighbors = KNN)\n#     model.fit(embeddings)\n#     distances, indices = model.kneighbors(embeddings)\n#     print(distances.shape)\n#     predictions = []\n#     for k in tqdm(range(embeddings.shape[0])):\n#         if isImage:\n#             idx = np.where(distances[k,] < 3.1)[0]\n#         else:\n#             idx = np.where(distances[k,] < 0.70)[0]\n#         ids = indices[k,idx]\n#         posting_ids = df['posting_id'].iloc[ids].values\n#         predictions.append(posting_ids)\n        \n#     del model, distances, indices\n#     gc.collect()\n#     return df, predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:36:10.844425Z","iopub.execute_input":"2022-04-16T20:36:10.844701Z","iopub.status.idle":"2022-04-16T20:36:10.853334Z","shell.execute_reply.started":"2022-04-16T20:36:10.844664Z","shell.execute_reply":"2022-04-16T20:36:10.852634Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# get nearest neighbors distances and index information\n\ndef get_knn_model(embeddings,KNN=50, metric='cosine'):\n    knnModel = NearestNeighbors(n_neighbors=KNN,metric=metric)\n    knnModel.fit(embeddings)\n#         distances, indices = knnModel.kneighbors(image_embeddings)\n    \n    return knnModel\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:36:10.855030Z","iopub.execute_input":"2022-04-16T20:36:10.855373Z","iopub.status.idle":"2022-04-16T20:36:10.861458Z","shell.execute_reply.started":"2022-04-16T20:36:10.855330Z","shell.execute_reply":"2022-04-16T20:36:10.860648Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# def search_similar_images(queryFeatures, index, maxResults=5):\n#     results=[]\n    \n#     # loop over our index\n#     for i in range(0, len(index[\"features\"])):\n#         # compute the  distance euclidean between our query features\n#         # and the features for the current image in our index, then\n#         dist = euclidean(queryFeatures, index[\"features\"][i])\n#         results.append((dist, index['indexes'][i]))\n\n#     # sort the results and grab the top ones\n#     results = sorted(results)[:maxResults]\n\n#     # return the list of results\n#     return results","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:36:10.862826Z","iopub.execute_input":"2022-04-16T20:36:10.863958Z","iopub.status.idle":"2022-04-16T20:36:10.869360Z","shell.execute_reply.started":"2022-04-16T20:36:10.863919Z","shell.execute_reply":"2022-04-16T20:36:10.868527Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/shopee-product-matching/train.csv')\n# get  Training image  path\ntrain_image_paths = get_images_path(train_df, CFG.TRAIN_DIR)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:36:10.870616Z","iopub.execute_input":"2022-04-16T20:36:10.871632Z","iopub.status.idle":"2022-04-16T20:36:10.992737Z","shell.execute_reply.started":"2022-04-16T20:36:10.871592Z","shell.execute_reply":"2022-04-16T20:36:10.991832Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#  get Training Image embeddings and save it for later use\ntest_transform =get_test_transforms()\nshopee_model = getPretrainedModel(loss_module='ArcFace',model_path=CFG.model_path_arcface, device=CFG.device)\ntrain_image_embeddings = getEmbeddings(train_image_paths, shopee_model, transform=test_transform)\nnp.save(\"training_image_embeddings\", train_image_embeddings)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:36:10.996431Z","iopub.execute_input":"2022-04-16T20:36:10.996649Z","iopub.status.idle":"2022-04-16T20:51:34.863046Z","shell.execute_reply.started":"2022-04-16T20:36:10.996621Z","shell.execute_reply":"2022-04-16T20:51:34.862220Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Let's visualize our Model Results","metadata":{}},{"cell_type":"code","source":"\n\ndef plot_canvas(train, random=False, COLS=6, ROWS=4, path=CFG.TRAIN_DIR+\"/\", isRecommending=False):\n    \n    for k in range(ROWS): \n        plt.figure(figsize=(20,5))\n        for j in range(COLS): \n            if random: row = np.random.randint(0,len(train))\n            else: row = COLS*k + j \n            name = train.iloc[row,1]\n            title = train.iloc[row,3]\n            posting_id = train.iloc[row,0]\n            label_g = train.iloc[row,4]\n            if isRecommending:\n                if j == 0 and row==0 :\n                    title_with_return = \"Query Image \\n\"\n                    title_with_return += \"-\"*20 + \"\\n Title :\"\n                else:\n                    title_with_return = \"Recommended Image {} \\n\".format(row)\n                    title_with_return += \"-\"*20 + \"\\n Title : \"\n            else:\n                title_with_return = \"Title : \"\n            for i,ch in enumerate(title):\n                title_with_return += ch\n                if (i!=0)&(i%20==0): title_with_return += '\\n'\n#             title_with_return += \" id:  \"+ posting_id[6:] + \"\\n\"\n#             title_with_return += \" label:  \"+ str(label_g) + \"\\n\"\n            img = cv2.imread(path+name)\n            img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB )\n            plt.subplot(1,COLS,j+1)\n            plt.title(title_with_return)\n            plt.axis('off')\n            plt.imshow(img)\n        plt.show()\n        \nplot_canvas(train_df,random=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T21:15:02.487877Z","iopub.execute_input":"2022-04-16T21:15:02.488138Z","iopub.status.idle":"2022-04-16T21:15:05.086083Z","shell.execute_reply.started":"2022-04-16T21:15:02.488108Z","shell.execute_reply":"2022-04-16T21:15:05.085434Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"# Visualize Top k Recommendation","metadata":{}},{"cell_type":"code","source":"def get_neighbors( train_embeddings, query_embeddings,KNN=50, metric_param='cosine'):\n    # we can get top neighbors based on different distance metric,in our case we are using \n    # cosine and euclidean metric\n    if metric_param == 'cosine':\n        # fit cosine distance medal on train image embddings\n        cosine_knnModel = get_knn_model(train_embeddings, KNN=KNN, metric='cosine')\n        # get top k neighbors distances and indices given metric for query embeddings\n        distances, indices = cosine_knnModel.kneighbors(query_embeddings)\n\n    else:\n        # fit euclidean distance modal on image embeddings\n        eucl_knnModel = get_knn_model(train_embeddings, KNN=KNN, metric='minkowski')\n        # get top k neighbors distances and indices given metric for query embeddings\n        distances, indices = eucl_knnModel.kneighbors(query_embeddings)\n    \n    return distances, indices\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:51:38.232508Z","iopub.execute_input":"2022-04-16T20:51:38.232953Z","iopub.status.idle":"2022-04-16T20:51:38.239769Z","shell.execute_reply.started":"2022-04-16T20:51:38.232916Z","shell.execute_reply":"2022-04-16T20:51:38.239027Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"metric_param = 'cosine'\ncosine_distances, cosine_indices = get_neighbors(\n                                    train_embeddings = train_image_embeddings,\n                                    query_embeddings = train_image_embeddings,\n                                    KNN=50,\n                                    metric_param='cosine'\n                                )\n\nnp.save('cosine_distance_training_f1_0.9', cosine_distances)\n\nnp.save('top_50_similar_cosine_indices_training_f1_0.9', cosine_indices)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:51:38.241183Z","iopub.execute_input":"2022-04-16T20:51:38.241512Z","iopub.status.idle":"2022-04-16T20:51:39.553469Z","shell.execute_reply.started":"2022-04-16T20:51:38.241475Z","shell.execute_reply":"2022-04-16T20:51:39.552619Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# 25530 # 28199,28210 # train_df[train_df['label_group']==994676122] 1980894329","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:51:39.554818Z","iopub.execute_input":"2022-04-16T20:51:39.555308Z","iopub.status.idle":"2022-04-16T20:51:39.559169Z","shell.execute_reply.started":"2022-04-16T20:51:39.555269Z","shell.execute_reply":"2022-04-16T20:51:39.558494Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Visualize Results based on Cosine Distance","metadata":{}},{"cell_type":"code","source":"testing_sample_id =[505,506,509,3,9,12,32,33,36,37,38,94,29096,29093,13]\n","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:51:39.560441Z","iopub.execute_input":"2022-04-16T20:51:39.561244Z","iopub.status.idle":"2022-04-16T20:51:39.569910Z","shell.execute_reply.started":"2022-04-16T20:51:39.561204Z","shell.execute_reply":"2022-04-16T20:51:39.568966Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"for k in (testing_sample_id):\n    plt.figure(figsize=(20,3))\n    plt.plot(np.arange(50),cupy.asnumpy(cosine_distances[k,]),'o-')\n    plt.title('Image {} Distance From Train Row {} to Other Train Rows'.format('cosine',k),size=16)\n    plt.ylabel('{} Distance to Train Row {}'.format('cosine', k),size=14)\n    plt.xlabel('Index Sorted by Distance to Train Row {}'.format(k),size=14)\n    plt.show()\n    \n    cluster = train_df.loc[cupy.asnumpy(cosine_indices[k,:8])] \n    plot_canvas(cluster, random=False, ROWS=2, COLS=4, isRecommending=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T21:15:09.173057Z","iopub.execute_input":"2022-04-16T21:15:09.173847Z","iopub.status.idle":"2022-04-16T21:15:31.532118Z","shell.execute_reply.started":"2022-04-16T21:15:09.173803Z","shell.execute_reply":"2022-04-16T21:15:31.531414Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"### Visualize Results based on Euclidean Distance","metadata":{}},{"cell_type":"code","source":"\neuc_distances, euc_indices = get_neighbors(\n                                    train_embeddings = train_image_embeddings,\n                                    query_embeddings = train_image_embeddings,\n                                    KNN=50,\n                                    metric_param='euclidean'\n                                )\n\nnp.save('euclidean_distance_training_f1_0.9', euc_distances)\n\nnp.save('top_50_similar_euclidean_indices_training_f1_0.9', euc_indices)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:51:53.352323Z","iopub.execute_input":"2022-04-16T20:51:53.352959Z","iopub.status.idle":"2022-04-16T20:51:54.106979Z","shell.execute_reply.started":"2022-04-16T20:51:53.352917Z","shell.execute_reply":"2022-04-16T20:51:54.106098Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"for k in (testing_sample_id):\n    plt.figure(figsize=(20,3))\n    plt.plot(np.arange(50),cupy.asnumpy(euc_distances[k,]),'o-')\n    plt.title('Image {} Distance From Train Row {} to Other Train Rows'.format('euclidean',k),size=16)\n    plt.ylabel('{} Distance to Train Row {}'.format('euclidean', k),size=14)\n    plt.xlabel('Index Sorted by Distance to Train Row {}'.format(k),size=14)\n    plt.show()\n    \n    cluster = train_df.loc[cupy.asnumpy(euc_indices[k,:8])] \n    plot_canvas(cluster, random=False, ROWS=2, COLS=4,isRecommending=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T21:15:54.181986Z","iopub.execute_input":"2022-04-16T21:15:54.182248Z","iopub.status.idle":"2022-04-16T21:16:15.644111Z","shell.execute_reply.started":"2022-04-16T21:15:54.182215Z","shell.execute_reply":"2022-04-16T21:16:15.643448Z"},"trusted":true},"execution_count":47,"outputs":[]}]}